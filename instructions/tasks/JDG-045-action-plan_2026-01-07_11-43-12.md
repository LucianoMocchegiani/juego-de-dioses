# JDG-045 - Sistema de Logging de Rendimiento (RAM, GPU, CPU)

## Descripción de la Tarea

Implementar un sistema de logging de rendimiento estructurado que monitoree y registre métricas de recursos del sistema (RAM, GPU, CPU) tanto en backend como en frontend. El sistema debe proporcionar visibilidad del consumo de recursos para facilitar la identificación de problemas de rendimiento y memory leaks.

**Comportamiento actual:**
- Backend: No existe sistema de logging de rendimiento
- Frontend: Existe sistema básico de métricas (`DebugMetrics`) que solo mide frame time y tiempos de sistemas ECS
- No se monitorean métricas de memoria, GPU, CPU, o pool de conexiones de base de datos

**Comportamiento esperado:**
- Backend: Sistema que loguea periódicamente métricas de memoria (RSS, heap), CPU (load average), y pool de conexiones (total, idle, waiting) en formato JSON estructurado
- Frontend: Extensión del sistema de debugger existente para incluir métricas de memoria, GPU, y CPU, mostrándolas en la interfaz de debug (F6)

## Criterios de Aceptación

1. ✅ Backend loguea métricas de rendimiento periódicamente (configurable, default: cada 30 segundos)
2. ✅ Backend incluye métricas de memoria (RSS, heap total, heap usado) en formato legible (MB)
3. ✅ Backend incluye métricas de CPU (load average 1m)
4. ✅ Backend incluye métricas del pool de conexiones de PostgreSQL (total, idle, waiting)
5. ✅ Backend usa formato de log estructurado (JSON) similar al ejemplo de NestJS
6. ✅ Frontend extiende el sistema de debugger existente con métricas de memoria
7. ✅ Frontend incluye métricas de GPU (si están disponibles desde Three.js)
8. ✅ Frontend muestra métricas en la interfaz de debug (F6) cuando está habilitada
9. ✅ Frontend puede loguear métricas a consola con formato estructurado (opcional)
10. ✅ El sistema no degrada significativamente el rendimiento (< 1% de overhead)
11. ✅ Las métricas se pueden habilitar/deshabilitar mediante configuración

## Contexto del Proyecto

### Proyecto(s) Afectado(s)
- [x] Backend (FastAPI)
- [x] Frontend (Three.js)
- [ ] Base de Datos (PostgreSQL)
- [ ] Cache (Redis)
- [ ] Docker/Infraestructura

### Tecnologías Involucradas
- Backend: Python 3.11, FastAPI, Uvicorn, asyncpg, psutil
- Frontend: HTML5, JavaScript ES6+, Three.js
- Base de datos: PostgreSQL 16
- Cache: Redis 7
- Logging: Python logging estándar, console.log estructurado

## Pasos de Implementación

### Paso 1: Configurar Dependencias del Backend

**Descripción:**
Agregar `psutil` a las dependencias del backend para obtener métricas del sistema (memoria, CPU).

**Archivos a modificar/crear:**
- `backend/requirements.txt`

**Detalles de implementación:**
```txt
# Agregar a requirements.txt
psutil>=5.9.0
```

**Notas:**
- `psutil` es una librería multiplataforma para obtener información del sistema
- Compatible con Windows, Linux y macOS
- No requiere permisos especiales para métricas básicas

**Recursos útiles:**
- Documentación de psutil: https://psutil.readthedocs.io/

---

### Paso 2: Crear Configuración de Performance (Backend)

**Descripción:**
Crear archivo de configuración para el sistema de monitoreo de rendimiento.

**Archivos a modificar/crear:**
- `backend/src/config/performance_config.py`
- `backend/src/config/__init__.py` (actualizar exports)

**Detalles de implementación:**
```python
# backend/src/config/performance_config.py
"""
Configuración del Sistema de Monitoreo de Rendimiento
"""

# Intervalo de logging en segundos
PERFORMANCE_LOG_INTERVAL = 30.0  # Default: 30 segundos

# Habilitar/deshabilitar logging de rendimiento
PERFORMANCE_LOG_ENABLED = True

# Nivel de log para métricas de rendimiento
PERFORMANCE_LOG_LEVEL = "DEBUG"  # DEBUG, INFO, WARNING, ERROR

# Diccionario de configuración
PERFORMANCE_CONFIG = {
    'LOG_INTERVAL': PERFORMANCE_LOG_INTERVAL,
    'LOG_ENABLED': PERFORMANCE_LOG_ENABLED,
    'LOG_LEVEL': PERFORMANCE_LOG_LEVEL
}
```

**Notas:**
- La configuración debe ser fácil de modificar
- Considerar usar variables de entorno en el futuro
- **⚠️ READMEs:** Si se crea una nueva carpeta o módulo, crear/actualizar README.md

---

### Paso 3: Crear Servicio de Monitoreo de Rendimiento (Backend)

**Descripción:**
Crear el servicio principal que recopila y loguea métricas de rendimiento del backend.

**Archivos a modificar/crear:**
- `backend/src/services/performance_monitor_service.py`
- `backend/src/services/__init__.py` (actualizar exports)

**Detalles de implementación:**
```python
# backend/src/services/performance_monitor_service.py
"""
Servicio de Monitoreo de Rendimiento del Backend
"""
import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, Optional
import psutil
from src.config.performance_config import PERFORMANCE_CONFIG
from src.database.connection import get_db_pool  # Ajustar según estructura real

logger = logging.getLogger(__name__)

class PerformanceMonitorService:
    """
    Servicio para monitorear y loguear métricas de rendimiento del sistema
    """
    
    def __init__(
        self,
        interval: float = None,
        enabled: bool = None
    ):
        """
        Args:
            interval: Intervalo de logging en segundos (default: desde config)
            enabled: Habilitar/deshabilitar monitoreo (default: desde config)
        """
        self.interval = interval or PERFORMANCE_CONFIG['LOG_INTERVAL']
        self.enabled = enabled if enabled is not None else PERFORMANCE_CONFIG['LOG_ENABLED']
        self._task: Optional[asyncio.Task] = None
        self._running = False
    
    def start(self):
        """Iniciar monitoreo en background task"""
        if not self.enabled:
            logger.debug("Performance monitoring is disabled")
            return
        
        if self._running:
            logger.warning("Performance monitoring is already running")
            return
        
        self._running = True
        self._task = asyncio.create_task(self._monitor_loop())
        logger.info(f"Performance monitoring started (interval: {self.interval}s)")
    
    def stop(self):
        """Detener monitoreo"""
        if not self._running:
            return
        
        self._running = False
        if self._task:
            self._task.cancel()
        logger.info("Performance monitoring stopped")
    
    async def _monitor_loop(self):
        """Loop principal de monitoreo"""
        try:
            while self._running:
                await asyncio.sleep(self.interval)
                if self._running:
                    await self._collect_and_log()
        except asyncio.CancelledError:
            logger.debug("Performance monitoring loop cancelled")
        except Exception as e:
            logger.error(f"Error in performance monitoring loop: {e}")
    
    async def _collect_and_log(self):
        """Recopilar y loguear métricas"""
        try:
            metrics = await self.collect_metrics()
            formatted = self.format_metrics(metrics)
            self.log_metrics(formatted)
        except Exception as e:
            logger.error(f"Error collecting performance metrics: {e}")
    
    async def collect_metrics(self) -> Dict:
        """
        Recopilar todas las métricas del sistema
        
        Returns:
            Diccionario con todas las métricas
        """
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "memory": self._collect_memory_metrics(),
            "cpu": self._collect_cpu_metrics(),
            "dbPool": await self._collect_db_pool_metrics()
        }
    
    def _collect_memory_metrics(self) -> Dict:
        """Recopilar métricas de memoria"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_percent = process.memory_percent()
            
            return {
                "rss": f"{memory_info.rss / 1024 / 1024:.2f} MB",
                "heapTotal": f"{memory_info.heap_total / 1024 / 1024:.2f} MB" if hasattr(memory_info, 'heap_total') else "N/A",
                "heapUsed": f"{memory_info.heap_used / 1024 / 1024:.2f} MB" if hasattr(memory_info, 'heap_used') else "N/A",
                "percent": f"{memory_percent:.2f}%"
            }
        except Exception as e:
            logger.warning(f"Error collecting memory metrics: {e}")
            return {"error": str(e)}
    
    def _collect_cpu_metrics(self) -> Dict:
        """Recopilar métricas de CPU"""
        try:
            load_avg = psutil.getloadavg()
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            return {
                "loadAvg1m": f"{load_avg[0]:.2f}",
                "loadAvg5m": f"{load_avg[1]:.2f}",
                "loadAvg15m": f"{load_avg[2]:.2f}",
                "percent": f"{cpu_percent:.2f}%"
            }
        except Exception as e:
            logger.warning(f"Error collecting CPU metrics: {e}")
            return {"error": str(e)}
    
    async def _collect_db_pool_metrics(self) -> Dict:
        """Recopilar métricas del pool de conexiones de PostgreSQL"""
        try:
            pool = get_db_pool()  # Ajustar según estructura real
            if pool is None:
                return {"error": "Database pool not initialized"}
            
            stats = pool.get_stats()
            
            return {
                "totalConnections": stats.get("size", 0),
                "idleConnections": stats.get("idle", 0),
                "waitingRequests": stats.get("waiting", 0)
            }
        except Exception as e:
            logger.warning(f"Error collecting DB pool metrics: {e}")
            return {"error": str(e)}
    
    def format_metrics(self, metrics: Dict) -> str:
        """
        Formatear métricas para logging
        
        Args:
            metrics: Diccionario con métricas
            
        Returns:
            String JSON formateado
        """
        return json.dumps(metrics, indent=2)
    
    def log_metrics(self, formatted_metrics: str):
        """
        Loguear métricas con formato estructurado
        
        Args:
            formatted_metrics: Métricas formateadas como JSON string
        """
        log_level = PERFORMANCE_CONFIG['LOG_LEVEL'].upper()
        if log_level == "DEBUG":
            logger.debug(f"[RUNTIME STATS] {formatted_metrics}")
        elif log_level == "INFO":
            logger.info(f"[RUNTIME STATS] {formatted_metrics}")
        elif log_level == "WARNING":
            logger.warning(f"[RUNTIME STATS] {formatted_metrics}")
        elif log_level == "ERROR":
            logger.error(f"[RUNTIME STATS] {formatted_metrics}")
```

**Notas:**
- El servicio debe manejar errores gracefully (no debe fallar si psutil no está disponible)
- Usar asyncio.create_task para ejecutar en background
- El formato de log debe ser similar al ejemplo de NestJS
- Ajustar `get_db_pool()` según la estructura real del proyecto
- **⚠️ READMEs:** Actualizar README.md de services/ si existe

---

### Paso 4: Integrar Servicio de Monitoreo en Main (Backend)

**Descripción:**
Integrar el servicio de monitoreo en el punto de entrada de la aplicación (main.py) para que se inicie automáticamente.

**Archivos a modificar/crear:**
- `backend/src/main.py`

**Detalles de implementación:**
```python
# En main.py, agregar al inicio del archivo:
from src.services.performance_monitor_service import PerformanceMonitorService

# En la función de startup (o donde se inicializa la app):
@app.on_event("startup")
async def startup_event():
    # ... código existente ...
    
    # Iniciar monitoreo de rendimiento
    performance_monitor = PerformanceMonitorService()
    performance_monitor.start()
    app.state.performance_monitor = performance_monitor  # Guardar referencia

@app.on_event("shutdown")
async def shutdown_event():
    # ... código existente ...
    
    # Detener monitoreo de rendimiento
    if hasattr(app.state, 'performance_monitor'):
        app.state.performance_monitor.stop()
```

**Notas:**
- El servicio debe iniciarse después de que la app esté lista
- Guardar referencia en app.state para poder detenerlo en shutdown
- Verificar que no interfiere con el código existente

---

### Paso 5: Extender DebugMetrics del Frontend con Métricas de Memoria

**Descripción:**
Extender la clase `DebugMetrics` existente para incluir métricas de memoria del navegador.

**Archivos a modificar/crear:**
- `frontend/src/debug/metrics.js`

**Detalles de implementación:**
```javascript
// En metrics.js, agregar método para obtener métricas de memoria:

/**
 * Obtener métricas de memoria del navegador
 * @returns {Object|null} Métricas de memoria o null si no están disponibles
 */
getMemoryMetrics() {
    if (!this.enabled) return null;
    
    // performance.memory puede no estar disponible en todos los navegadores
    if (!performance.memory) {
        return null;
    }
    
    const memory = performance.memory;
    
    return {
        heapTotal: this._formatBytes(memory.totalJSHeapSize),
        heapUsed: this._formatBytes(memory.usedJSHeapSize),
        heapLimit: this._formatBytes(memory.jsHeapSizeLimit),
        percent: ((memory.usedJSHeapSize / memory.jsHeapSizeLimit) * 100).toFixed(2) + "%"
    };
}

/**
 * Formatear bytes a formato legible (MB, GB, etc.)
 * @param {number} bytes - Bytes a formatear
 * @returns {string} String formateado
 */
_formatBytes(bytes) {
    if (bytes === 0) return "0 B";
    const k = 1024;
    const sizes = ["B", "KB", "MB", "GB"];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    return (bytes / Math.pow(k, i)).toFixed(2) + " " + sizes[i];
}

// Extender getStats() para incluir memoria:
getStats() {
    if (!this.enabled) return null;
    
    const stats = {
        frameTime: {
            // ... código existente ...
        },
        memory: this.getMemoryMetrics(),  // Agregar esta línea
        systems: {
            // ... código existente ...
        }
    };
    
    return stats;
}
```

**Notas:**
- `performance.memory` puede no estar disponible en todos los navegadores (es una API no estándar)
- El método debe retornar null si no está disponible (no debe fallar)
- Formatear bytes a formato legible (MB, GB)

---

### Paso 6: Agregar Métricas de GPU al Frontend

**Descripción:**
Extender `DebugMetrics` para incluir métricas de GPU desde el renderer de Three.js.

**Archivos a modificar/crear:**
- `frontend/src/debug/metrics.js`

**Detalles de implementación:**
```javascript
// En metrics.js, agregar método para obtener métricas de GPU:

/**
 * Obtener métricas de GPU desde Three.js renderer
 * @param {THREE.WebGLRenderer} renderer - Renderer de Three.js (opcional)
 * @returns {Object|null} Métricas de GPU o null si no están disponibles
 */
getGPUMetrics(renderer = null) {
    if (!this.enabled) return null;
    
    // Si no se proporciona renderer, intentar obtenerlo del ECS o scene
    if (!renderer) {
        // Intentar obtener renderer del contexto (ajustar según estructura real)
        // renderer = this.ecs?.scene?.renderer?.renderer;
        return null; // Por ahora retornar null si no está disponible
    }
    
    if (!renderer.info) {
        return null;
    }
    
    const info = renderer.info;
    
    return {
        drawCalls: info.render.calls || 0,
        triangles: info.render.triangles || 0,
        points: info.render.points || 0,
        lines: info.render.lines || 0,
        geometries: info.memory.geometries || 0,
        textures: info.memory.textures || 0,
        programs: info.programs?.length || 0
    };
}

// Extender getStats() para incluir GPU:
getStats(renderer = null) {
    if (!this.enabled) return null;
    
    const stats = {
        frameTime: {
            // ... código existente ...
        },
        memory: this.getMemoryMetrics(),
        gpu: this.getGPUMetrics(renderer),  // Agregar esta línea
        systems: {
            // ... código existente ...
        }
    };
    
    return stats;
}
```

**Notas:**
- El renderer debe pasarse como parámetro o obtenerse del contexto
- `renderer.info` contiene información detallada del renderer
- Ajustar según cómo se accede al renderer en el proyecto

---

### Paso 7: Crear PerformanceLogger para Frontend

**Descripción:**
Crear un módulo separado para logging estructurado de métricas de rendimiento en el frontend.

**Archivos a modificar/crear:**
- `frontend/src/debug/performance-logger.js`
- `frontend/src/debug/README.md` (actualizar)

**Detalles de implementación:**
```javascript
/**
 * Logger de Rendimiento para Frontend
 * Loguea métricas de rendimiento en formato estructurado (JSON)
 */
import { DEBUG_CONFIG } from '../config/debug-config.js';

export class PerformanceLogger {
    constructor(debugMetrics, renderer = null) {
        this.debugMetrics = debugMetrics;
        this.renderer = renderer;
        this.enabled = DEBUG_CONFIG.performanceLogging?.enabled ?? false;
        this.interval = DEBUG_CONFIG.performanceLogging?.interval ?? 30000; // 30 segundos
        this.intervalId = null;
    }
    
    /**
     * Iniciar logging periódico
     */
    start() {
        if (!this.enabled) return;
        
        this.stop(); // Asegurar que no hay otro intervalo activo
        
        this.intervalId = setInterval(() => {
            this.logMetrics();
        }, this.interval);
    }
    
    /**
     * Detener logging periódico
     */
    stop() {
        if (this.intervalId) {
            clearInterval(this.intervalId);
            this.intervalId = null;
        }
    }
    
    /**
     * Loguear métricas actuales
     */
    logMetrics() {
        if (!this.enabled) return;
        
        const stats = this.debugMetrics.getStats(this.renderer);
        if (!stats) return;
        
        const logData = {
            timestamp: new Date().toISOString(),
            frameTime: stats.frameTime,
            memory: stats.memory,
            gpu: stats.gpu,
            systems: stats.systems
        };
        
        console.log('[RUNTIME STATS]', JSON.stringify(logData, null, 2));
    }
    
    /**
     * Habilitar/deshabilitar logging
     */
    setEnabled(enabled) {
        this.enabled = enabled;
        if (enabled) {
            this.start();
        } else {
            this.stop();
        }
    }
}
```

**Notas:**
- El logger debe ser opcional y configurable
- Usar `setInterval` para logging periódico
- Formato JSON estructurado similar al backend
- **⚠️ READMEs:** Actualizar README.md de debug/ explicando el nuevo módulo

---

### Paso 8: Actualizar Configuración de Debug del Frontend

**Descripción:**
Agregar configuración para el logging de rendimiento en el frontend.

**Archivos a modificar/crear:**
- `frontend/src/config/debug-config.js`

**Detalles de implementación:**
```javascript
// En debug-config.js, agregar configuración de performance logging:

export const DEBUG_CONFIG = {
    // ... configuración existente ...
    
    performanceLogging: {
        enabled: false,  // Deshabilitado por defecto
        interval: 30000  // 30 segundos
    }
};
```

**Notas:**
- El logging debe estar deshabilitado por defecto para no saturar la consola
- El intervalo debe ser configurable

---

### Paso 9: Integrar Métricas en Interfaz de Debug (F6)

**Descripción:**
Mostrar las nuevas métricas (memoria, GPU) en la interfaz de debug existente (F6).

**Archivos a modificar/crear:**
- `frontend/src/interfaces/debug-interface.js`

**Detalles de implementación:**
```javascript
// En debug-interface.js, agregar sección para métricas de rendimiento:

// En el método que muestra métricas, agregar:
displayPerformanceMetrics(stats) {
    if (!stats) return;
    
    // Sección de Memoria
    if (stats.memory) {
        this.addMetricSection('Memory', {
            'Heap Total': stats.memory.heapTotal,
            'Heap Used': stats.memory.heapUsed,
            'Heap Limit': stats.memory.heapLimit,
            'Percent': stats.memory.percent
        });
    }
    
    // Sección de GPU
    if (stats.gpu) {
        this.addMetricSection('GPU', {
            'Draw Calls': stats.gpu.drawCalls,
            'Triangles': stats.gpu.triangles,
            'Geometries': stats.gpu.geometries,
            'Textures': stats.gpu.textures
        });
    }
}

// Llamar este método cuando se actualicen las métricas
```

**Notas:**
- Integrar con la interfaz existente sin romper funcionalidad
- Mostrar métricas solo si están disponibles
- Actualizar periódicamente (no en cada frame)

---

### Paso 10: Integrar PerformanceLogger en App Principal

**Descripción:**
Integrar el `PerformanceLogger` en la aplicación principal para que se inicialice y pueda iniciarse/pararse.

**Archivos a modificar/crear:**
- `frontend/src/app.js` (o donde se inicializa la app)

**Detalles de implementación:**
```javascript
// En app.js, importar y crear instancia:

import { PerformanceLogger } from './debug/performance-logger.js';

// En el constructor o método de inicialización:
constructor() {
    // ... código existente ...
    
    // Crear performance logger (opcional, solo si está habilitado)
    if (DEBUG_CONFIG.performanceLogging?.enabled) {
        this.performanceLogger = new PerformanceLogger(
            this.debugMetrics,
            this.scene?.renderer?.renderer  // Ajustar según estructura real
        );
        this.performanceLogger.start();
    }
}
```

**Notas:**
- El logger debe ser opcional y solo iniciarse si está habilitado
- Ajustar cómo se obtiene el renderer según la estructura real del proyecto

---

### Paso 11: Testing y Verificación

**Descripción:**
Verificar que el sistema funciona correctamente en ambos lados (backend y frontend).

**Archivos a modificar/crear:**
- Ninguno (solo testing)

**Detalles de implementación:**
1. **Backend:**
   - Verificar que los logs aparecen cada 30 segundos (o intervalo configurado)
   - Verificar que el formato JSON es correcto
   - Verificar que incluye todas las métricas (memoria, CPU, DB pool)
   - Verificar que funciona aunque psutil no esté disponible (fallback)

2. **Frontend:**
   - Abrir interfaz de debug (F6)
   - Verificar que se muestran métricas de memoria y GPU
   - Verificar que las métricas se actualizan periódicamente
   - Verificar que no degrada el FPS del juego
   - Verificar que el logging a consola funciona (si está habilitado)

3. **Performance:**
   - Medir overhead del sistema (debe ser < 1%)
   - Verificar que no hay memory leaks en el sistema de monitoreo

**Notas:**
- Testing manual es suficiente para este feature
- Verificar en diferentes navegadores (Chrome, Firefox, Edge)

---

### Paso Final: Generar Descripción del Pull Request

**Descripción:**
Una vez completados todos los pasos anteriores y verificada la implementación, genera la descripción completa del Pull Request usando la regla `@pr-description.mdc`.

**Comando a ejecutar:**
```
@pr-description.mdc
```

**Resultado esperado:**
- Archivo generado: `JDG-045_pr-description_[FECHA-HORA].md` en `/instructions/prs/` (con fecha y hora obtenida de `Get-Date -Format "yyyy-MM-dd_HH-mm-ss"`)
- El archivo contendrá una descripción completa del PR lista para copiar y pegar en Git
- Incluirá: título, resumen, motivación, cambios técnicos, testing, referencias y riesgos

**Notas:**
- Este paso se ejecuta DESPUÉS de completar toda la implementación
- La descripción se genera automáticamente basándose en los cambios realizados
- El desarrollador solo necesita copiar y pegar el contenido en Git
- No editar manualmente a menos que sea estrictamente necesario

---

## Consideraciones Técnicas

### Performance
- El logging debe ejecutarse en background (no bloquear el hilo principal)
- Intervalos configurables para no saturar logs
- Sampling opcional para reducir overhead
- El sistema debe tener mínimo impacto en el rendimiento del juego (< 1%)

### Seguridad
- No se exponen datos sensibles en los logs
- Los logs de rendimiento no deben contener información de usuarios

### Casos Edge
- `psutil` puede no estar disponible → implementar fallback o advertencia
- `performance.memory` puede no estar disponible en algunos navegadores → retornar null
- El renderer puede no estar inicializado → manejar gracefully
- El pool de conexiones puede no estar inicializado → retornar error en métricas

### Compatibilidad
- Backend debe funcionar sin `psutil` (con advertencia)
- Frontend debe funcionar si `performance.memory` no está disponible
- No romper funcionalidad existente del debugger

## Patrones de Código a Usar

- **Backend (FastAPI)**: 
  - Servicio singleton para monitoreo
  - Background tasks con asyncio.create_task
  - Logging estructurado con formato JSON
  - Manejo de errores graceful

- **Frontend (Three.js)**: 
  - Extensión de clases existentes (no crear nuevas)
  - Integración con sistema de debugger existente
  - Formato JSON para logging estructurado
  - Opcionalidad y configurabilidad

## Dependencias

### Nuevas Dependencias
```txt
psutil>=5.9.0
```

### Variables de Entorno
- Ninguna nueva variable de entorno requerida (por ahora)

## Archivos Principales Involucrados

1. `backend/src/services/performance_monitor_service.py` - Servicio principal de monitoreo
2. `backend/src/config/performance_config.py` - Configuración de monitoreo
3. `backend/src/main.py` - Integración del servicio
4. `frontend/src/debug/metrics.js` - Extensión con nuevas métricas
5. `frontend/src/debug/performance-logger.js` - Logger estructurado
6. `frontend/src/interfaces/debug-interface.js` - Mostrar métricas en UI
7. `frontend/src/config/debug-config.js` - Configuración de logging

## Testing

### Tests a Crear/Modificar
- Testing manual es suficiente para este feature
- Verificar logs en consola (backend y frontend)
- Verificar interfaz de debug (F6)

### Escenarios de Prueba
1. Backend loguea métricas cada 30 segundos con formato correcto
2. Backend incluye todas las métricas requeridas (memoria, CPU, DB pool)
3. Frontend muestra métricas en interfaz de debug (F6)
4. Frontend loguea métricas a consola (si está habilitado)
5. El sistema no degrada rendimiento significativamente
6. El sistema funciona aunque algunas APIs no estén disponibles

## Deployment

### Orden de Deployment
1. Backend: Rebuild Docker image y restart container
2. Frontend: Actualizar archivos estáticos (si aplica)
3. Verificar en ambiente local con Docker Compose

### Verificación Post-Deployment
- [ ] Verificar logs del backend (deben aparecer métricas cada 30 segundos)
- [ ] Verificar frontend en navegador (abrir F6 y ver métricas)
- [ ] Verificar logs de Docker
- [ ] Verificar que no hay errores en consola

---

**Nota Final:** Este plan debe ejecutarse paso a paso, verificando cada paso antes de continuar con el siguiente. Si encuentras problemas o necesitas clarificación, consulta con el equipo antes de proceder.

**⚠️ IMPORTANTE:** El último paso del plan SIEMPRE debe ser "Generar Descripción del Pull Request" usando `@pr-description.mdc`. Esto genera automáticamente la descripción completa del PR lista para Git.
